### Prepare Next.js app

**Recommended**: To use a starter template, clone the [Auth0 AI samples](https://github.com/auth0-samples/auth0-ai-samples) repository:

```bash
git clone https://github.com/auth0-samples/auth0-ai-samples.git
cd auth0-ai-samples/authenticate-users/langchain-next-js
```

### Install dependencies

In the root directory of your project, install the following dependencies:

- `@langchain/langgraph`: The core [LangGraph](https://js.langchain.com/docs/langgraph) module.
- `@langchain/openai`: [OpenAI](https://js.langchain.com/docs/integrations/chat/openai) provider for LangChain.
- `langchain`: The core [LangChain](https://js.langchain.com) module.
- `zod`: TypeScript-first schema validation library.
- `langgraph-nextjs-api-passthrough`: API passthrough for LangGraph.

```bash
npm install @langchain/langgraph@0.3 @langchain/openai@0.6 langchain@0.3 zod@3 langgraph-nextjs-api-passthrough@0.1
```

### Pass credentials to the agent

First, you have to pass the access token from the user's session to the agent. First, create a helper function to get the access token from the session. Add the following function to `src/lib/auth0.ts`.

```ts src/lib/auth0.ts lines highlight={4-7}
//...

// Get the Access token from Auth0 session
export const getAccessToken = async () => {
  const session = await auth0.getSession();
  return session?.tokenSet?.accessToken;
};
```

Now, update the `/src/app/api/chat/[..._path]/route.ts` file to pass the access token to the agent.

```ts src/app/api/chat/[..._path]/route.ts lines highlight={3,19}
import { initApiPassthrough } from "langgraph-nextjs-api-passthrough";

import { getAccessToken } from "@/lib/auth0";

export const { GET, POST, PUT, PATCH, DELETE, OPTIONS, runtime } =
  initApiPassthrough({
    apiUrl: process.env.LANGGRAPH_API_URL,
    baseRoute: "chat/",
    bodyParameters: async (req, body) => {
      if (
        req.nextUrl.pathname.endsWith("/runs/stream") &&
        req.method === "POST"
      ) {
        return {
          ...body,
          config: {
            configurable: {
              _credentials: {
                accessToken: await getAccessToken(),
              },
            },
          },
        };
      }

      return body;
    },
  });
```

### Define a tool to call your API

In this step, you'll create a LangChain tool to make the first-party API call. The tool fetches an access token to call the API.

In this example, after taking in an Auth0 access token during user login, the tool returns the user profile of the currently logged-in user by calling the [/userinfo](https://auth0.com/docs/api/authentication/user-profile/get-user-info) endpoint.

```ts src/lib/tools/user-info.ts lines
import { tool } from "@langchain/core/tools";

export const getUserInfoTool = tool(
  async (_input, config?) => {
    // Access credentials from config
    const accessToken = config?.configurable?._credentials?.accessToken;
    if (!accessToken) {
      return "There is no user logged in.";
    }

    const response = await fetch(
      `https://${process.env.AUTH0_DOMAIN}/userinfo`,
      {
        headers: {
          Authorization: `Bearer ${accessToken}`,
        },
      }
    );

    if (response.ok) {
      return { result: await response.json() };
    }

    return "I couldn't verify your identity";
  },
  {
    name: "get_user_info",
    description: "Get information about the current logged in user.",
  }
);
```

### Add the tool to the AI agent

The AI agent processes and runs the user's request through the AI pipeline, including the tool call. Update the `/src/lib/agent.ts` file to add the tool to the agent.

```ts src/lib/agent.ts lines highlight={2,8}
//...
import { getUserInfoTool } from "./tools/user-info";

//... existing code

const tools = [
  //... existing tools
  getUserInfoTool,
];

//... existing code
```

You need an API Key from Open AI or another provider to use an LLM. Add that API key to your `.env.local` file:

```bash .env.local
# ...
# You can use any provider of your choice supported by Vercel AI
OPENAI_API_KEY="YOUR_API_KEY"
```

If you use another provider for your LLM, adjust the variable name in `.env.local` accordingly.

### Test your application

To test the application, run `npm run all:dev` and navigate to `http://localhost:3000` and interact with the AI agent. You can ask questions like `"who am I?"` to trigger the tool call and test whether it successfully retrieves information about the logged-in user.

```bash
User: who am I?
AI: It seems that there is no user currently logged in. If you need assistance with anything else, feel free to ask!

User: who am I?
AI: You are Deepu Sasidharan. Here are your details: - .........
```

That's it! You've successfully integrated first-party tool-calling into your project.

Explore [the example app on GitHub](https://github.com/auth0-samples/auth0-ai-samples/tree/main/call-apis-on-users-behalf/your-api/langchain-next-js).
